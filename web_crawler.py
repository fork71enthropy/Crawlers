"""
Web Crawler Ã‰ducatif
====================
Ce script montre comment construire un crawler web simple et respectueux.

IMPORTANT - Ã‰thique du crawling:
1. Toujours respecter le fichier robots.txt
2. Limiter la vitesse des requÃªtes (rate limiting)
3. Ne crawler que des sites pour lesquels vous avez la permission
4. Identifier votre bot avec un User-Agent appropriÃ©
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import time
import re


class SimpleCrawler:
    """Crawler web simple et Ã©ducatif"""
    
    def __init__(self, start_url, max_pages=10, delay=1):
        """
        Args:
            start_url: URL de dÃ©part
            max_pages: Nombre maximum de pages Ã  crawler
            delay: DÃ©lai entre chaque requÃªte (en secondes)
        """
        self.start_url = start_url
        self.max_pages = max_pages
        self.delay = delay
        self.visited = set()
        self.to_visit = deque([start_url])
        self.domain = urlparse(start_url).netloc
        
        # User-Agent poli et identifiable
        self.headers = {
            'User-Agent': 'EducationalCrawler/1.0 (Educational purposes)'
        }
    
    def is_valid_url(self, url):
        """VÃ©rifie si l'URL est valide et appartient au mÃªme domaine"""
        parsed = urlparse(url)
        return (
            bool(parsed.netloc) and 
            bool(parsed.scheme) and
            parsed.netloc == self.domain
        )
    
    def get_links(self, url, html):
        """Extrait tous les liens d'une page"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            # Convertir en URL absolue
            absolute_url = urljoin(url, href)
            
            if self.is_valid_url(absolute_url):
                links.append(absolute_url)
        
        return links
    
    def extract_text(self, html):
        """Extrait le texte principal de la page"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Supprimer scripts et styles
        for script in soup(['script', 'style']):
            script.decompose()
        
        # Extraire le texte
        text = soup.get_text()
        # Nettoyer les espaces multiples
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
    
    def crawl(self):
        """Lance le crawling"""
        print(f"ğŸ•·ï¸  DÃ©but du crawling de: {self.start_url}")
        print(f"ğŸ“Š Limite: {self.max_pages} pages\n")
        
        pages_data = []
        
        while self.to_visit and len(self.visited) < self.max_pages:
            url = self.to_visit.popleft()
            
            # Ã‰viter de visiter deux fois la mÃªme URL
            if url in self.visited:
                continue
            
            try:
                print(f"ğŸ“„ [{len(self.visited) + 1}/{self.max_pages}] Crawling: {url}")
                
                # Respecter le dÃ©lai entre les requÃªtes
                if self.visited:
                    time.sleep(self.delay)
                
                # RÃ©cupÃ©rer la page
                response = requests.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                
                # Marquer comme visitÃ©e
                self.visited.add(url)
                
                # Extraire les donnÃ©es
                html = response.text
                text = self.extract_text(html)
                links = self.get_links(url, html)
                
                # Sauvegarder les donnÃ©es
                pages_data.append({
                    'url': url,
                    'title': BeautifulSoup(html, 'html.parser').title.string if BeautifulSoup(html, 'html.parser').title else 'No title',
                    'text_length': len(text),
                    'links_found': len(links)
                })
                
                # Ajouter les nouveaux liens Ã  visiter
                for link in links:
                    if link not in self.visited and link not in self.to_visit:
                        self.to_visit.append(link)
                
                print(f"   âœ“ TrouvÃ© {len(links)} liens | Texte: {len(text)} caractÃ¨res")
                
            except requests.RequestException as e:
                print(f"   âœ— Erreur: {e}")
            except Exception as e:
                print(f"   âœ— Erreur inattendue: {e}")
        
        print(f"\nâœ… Crawling terminÃ©!")
        print(f"ğŸ“Š Pages visitÃ©es: {len(self.visited)}")
        print(f"ğŸ“‹ Pages en attente: {len(self.to_visit)}")
        
        return pages_data


class RobotsTxtChecker:
    """VÃ©rifie si le crawling est autorisÃ© selon robots.txt"""
    
    @staticmethod
    def can_fetch(url, user_agent='*'):
        """VÃ©rifie si l'URL peut Ãªtre crawlÃ©e selon robots.txt"""
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        
        try:
            response = requests.get(robots_url, timeout=5)
            if response.status_code == 200:
                print(f"\nğŸ“œ robots.txt trouvÃ© Ã  {robots_url}")
                print("PremiÃ¨re lignes:")
                print(response.text[:500])
                return True
            else:
                print(f"\nğŸ“œ Pas de robots.txt (status: {response.status_code})")
                return True
        except requests.RequestException:
            print(f"\nğŸ“œ Impossible d'accÃ©der Ã  robots.txt")
            return True


# ============================================================================
# EXEMPLES D'UTILISATION
# ============================================================================

def exemple_1_crawl_basique():
    """Exemple 1: Crawl basique d'un site"""
    print("=" * 70)
    print("EXEMPLE 1: Crawl basique")
    print("=" * 70)
    
    # Site d'exemple (remplacer par un site que vous avez la permission de crawler)
    url = "https://example.com"
    
    # VÃ©rifier robots.txt
    RobotsTxtChecker.can_fetch(url)
    
    # CrÃ©er et lancer le crawler
    crawler = SimpleCrawler(
        start_url=url,
        max_pages=5,
        delay=1  # 1 seconde entre chaque requÃªte
    )
    
    data = crawler.crawl()
    
    # Afficher les rÃ©sultats
    print("\nğŸ“Š RÃ‰SULTATS:")
    for i, page in enumerate(data, 1):
        print(f"\n{i}. {page['title']}")
        print(f"   URL: {page['url']}")
        print(f"   Texte: {page['text_length']} caractÃ¨res")
        print(f"   Liens: {page['links_found']}")


def exemple_2_extraction_specifique():
    """Exemple 2: Extraction d'informations spÃ©cifiques"""
    print("\n" + "=" * 70)
    print("EXEMPLE 2: Extraction d'informations spÃ©cifiques")
    print("=" * 70)
    
    url = "https://example.com"
    
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraire tous les titres
        print("\nğŸ“Œ TITRES (h1, h2, h3):")
        for i, heading in enumerate(soup.find_all(['h1', 'h2', 'h3']), 1):
            print(f"{i}. [{heading.name}] {heading.get_text().strip()}")
        
        # Extraire tous les liens
        print("\nğŸ”— LIENS:")
        for i, link in enumerate(soup.find_all('a', href=True)[:10], 1):
            print(f"{i}. {link.get_text().strip()[:50]} -> {link['href']}")
        
        # Extraire les images
        print("\nğŸ–¼ï¸  IMAGES:")
        for i, img in enumerate(soup.find_all('img')[:5], 1):
            print(f"{i}. {img.get('alt', 'No alt')} -> {img.get('src', 'No src')}")
            
    except Exception as e:
        print(f"Erreur: {e}")


def exemple_3_sitemap_generator():
    """Exemple 3: GÃ©nÃ©rer un sitemap simple"""
    print("\n" + "=" * 70)
    print("EXEMPLE 3: GÃ©nÃ©rateur de sitemap")
    print("=" * 70)
    
    url = "https://example.com"
    crawler = SimpleCrawler(start_url=url, max_pages=10, delay=1)
    data = crawler.crawl()
    
    # GÃ©nÃ©rer un sitemap simple
    sitemap = "SITEMAP\n" + "=" * 50 + "\n\n"
    for page in data:
        sitemap += f"â€¢ {page['title']}\n"
        sitemap += f"  {page['url']}\n\n"
    
    print(sitemap)
    
    # Sauvegarder dans un fichier
    with open('/home/claude/sitemap.txt', 'w', encoding='utf-8') as f:
        f.write(sitemap)
    print("ğŸ’¾ Sitemap sauvegardÃ© dans sitemap.txt")


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         WEB CRAWLER Ã‰DUCATIF - GUIDE D'UTILISATION             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸  RAPPEL IMPORTANT:
   â€¢ Ne crawlez que des sites pour lesquels vous avez la permission
   â€¢ Respectez toujours le fichier robots.txt
   â€¢ Utilisez des dÃ©lais raisonnables entre les requÃªtes
   â€¢ Identifiez votre crawler avec un User-Agent appropriÃ©

ğŸ“š Ce script contient 3 exemples:
   1. Crawl basique d'un site
   2. Extraction d'informations spÃ©cifiques
   3. GÃ©nÃ©ration d'un sitemap

DÃ©commentez l'exemple que vous souhaitez exÃ©cuter ci-dessous:
""")
    
    # DÃ©commentez l'exemple que vous voulez tester:
    # exemple_1_crawl_basique()
    # exemple_2_extraction_specifique()
    # exemple_3_sitemap_generator()
    
    print("\nğŸ’¡ Pour utiliser ce script:")
    print("   1. Installez les dÃ©pendances: pip install requests beautifulsoup4")
    print("   2. DÃ©commentez un exemple dans la section __main__")
    print("   3. Remplacez l'URL par un site que vous pouvez crawler lÃ©galement")
    print("   4. ExÃ©cutez: python web_crawler_educatif.py")
